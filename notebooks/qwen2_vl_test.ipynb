{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Gozumol - Qwen2-VL Testing Notebook\n",
        "\n",
        "This notebook demonstrates testing the Qwen2-VL model for visual assistance.\n",
        "\n",
        "**Purpose**: Test the model's ability to describe environments and provide navigation guidance for visually impaired users.\n",
        "\n",
        "## Requirements\n",
        "\n",
        "```bash\n",
        "transformers>=4.45.0\n",
        "torch>=2.0.0\n",
        "pillow>=10.0.0\n",
        "qwen-vl-utils\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install transformers>=4.45.0\n",
        "!pip install torch>=2.0.0\n",
        "!pip install pillow>=10.0.0\n",
        "!pip install qwen-vl-utils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Model and Processor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
        "import torch\n",
        "\n",
        "# Model configuration\n",
        "MODEL_ID = \"Qwen/Qwen2-VL-2B-Instruct\"\n",
        "\n",
        "# Load model\n",
        "print(\"Loading model...\")\n",
        "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "# Load processor\n",
        "print(\"Loading processor...\")\n",
        "processor = AutoProcessor.from_pretrained(MODEL_ID)\n",
        "\n",
        "print(\"Model loaded successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Define Vision Assistant Interface\n",
        "\n",
        "This function creates the appropriate message structure and processes inputs for the Qwen2-VL model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "from PIL import Image\n",
        "\n",
        "def qwen_vision_inference(\n",
        "    processor,\n",
        "    model,\n",
        "    image: Image.Image,\n",
        "    user_prompt: str,\n",
        "    system_prompt: str = None,\n",
        "    max_new_tokens: int = 1024\n",
        "):\n",
        "    \"\"\"\n",
        "    Generate a response from the Qwen2-VL model.\n",
        "    \n",
        "    Parameters:\n",
        "        processor: The AutoProcessor for the model\n",
        "        model: The loaded Qwen2-VL model\n",
        "        image: PIL Image to analyze\n",
        "        user_prompt: User's question/instruction\n",
        "        system_prompt: Optional system message\n",
        "        max_new_tokens: Maximum tokens to generate\n",
        "    \n",
        "    Returns:\n",
        "        Tuple of (response_text, timing_info)\n",
        "    \"\"\"\n",
        "    # Build messages\n",
        "    messages = []\n",
        "    \n",
        "    if system_prompt:\n",
        "        messages.append({\n",
        "            \"role\": \"system\",\n",
        "            \"content\": system_prompt\n",
        "        })\n",
        "    \n",
        "    messages.append({\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"image\"},\n",
        "            {\"type\": \"text\", \"text\": user_prompt}\n",
        "        ]\n",
        "    })\n",
        "    \n",
        "    # Track timing\n",
        "    timing_info = {}\n",
        "    \n",
        "    # Process inputs\n",
        "    timing_info[\"start_processor\"] = time.time()\n",
        "    text_prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
        "    inputs = processor(\n",
        "        text=[text_prompt],\n",
        "        images=[image],\n",
        "        padding=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    inputs = inputs.to(model.device)\n",
        "    timing_info[\"processor_time\"] = time.time() - timing_info[\"start_processor\"]\n",
        "    \n",
        "    # Generate response\n",
        "    timing_info[\"start_generation\"] = time.time()\n",
        "    output_ids = model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
        "    timing_info[\"generation_time\"] = time.time() - timing_info[\"start_generation\"]\n",
        "    \n",
        "    # Extract generated tokens only\n",
        "    generated_ids = [\n",
        "        output_ids[len(input_ids):]\n",
        "        for input_ids, output_ids in zip(inputs.input_ids, output_ids)\n",
        "    ]\n",
        "    \n",
        "    # Decode response\n",
        "    timing_info[\"start_decode\"] = time.time()\n",
        "    response = processor.batch_decode(\n",
        "        generated_ids,\n",
        "        skip_special_tokens=True,\n",
        "        clean_up_tokenization_spaces=True\n",
        "    )[0]\n",
        "    timing_info[\"decode_time\"] = time.time() - timing_info[\"start_decode\"]\n",
        "    \n",
        "    # Calculate total time\n",
        "    timing_info[\"total_time\"] = (\n",
        "        timing_info[\"processor_time\"] +\n",
        "        timing_info[\"generation_time\"] +\n",
        "        timing_info[\"decode_time\"]\n",
        "    )\n",
        "    \n",
        "    return response, timing_info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Display Helper Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "def run_and_display(image, user_prompt, system_prompt=None, test_name=\"Test\"):\n",
        "    \"\"\"\n",
        "    Run inference and display results in a formatted way.\n",
        "    \n",
        "    Parameters:\n",
        "        image: PIL Image to analyze\n",
        "        user_prompt: User's prompt\n",
        "        system_prompt: Optional system prompt\n",
        "        test_name: Name to display for this test\n",
        "    \n",
        "    Returns:\n",
        "        Tuple of (response, timing)\n",
        "    \"\"\"\n",
        "    display(Markdown(f\"## {test_name}\"))\n",
        "    \n",
        "    # Show the user prompt\n",
        "    display(Markdown(f\"**User Prompt:** {user_prompt[:200]}...\"))\n",
        "    \n",
        "    # Run inference\n",
        "    response, timing = qwen_vision_inference(\n",
        "        processor, model, image, user_prompt, system_prompt\n",
        "    )\n",
        "    \n",
        "    # Display response\n",
        "    display(Markdown(\"### Response:\"))\n",
        "    display(Markdown(f\"> {response}\"))\n",
        "    \n",
        "    # Display timing\n",
        "    display(Markdown(\"### Timing:\"))\n",
        "    display(Markdown(f\"\"\"\n",
        "- Processor time: {timing['processor_time']:.3f}s\n",
        "- Generation time: {timing['generation_time']:.3f}s\n",
        "- Decode time: {timing['decode_time']:.3f}s\n",
        "- **Total time: {timing['total_time']:.3f}s**\n",
        "    \"\"\"))\n",
        "    \n",
        "    return response, timing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Visual Assistance Prompts\n",
        "\n",
        "These are the prompts optimized for visually impaired navigation assistance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# System prompt for visual assistance\n",
        "SYSTEM_PROMPT = \"\"\"You are an AI assistant guiding a visually impaired person through a live camera feed. \n",
        "Your role is to be a calm, trustworthy companion who helps them navigate safely and independently.\"\"\"\n",
        "\n",
        "# User prompt for environment description\n",
        "USER_PROMPT = \"\"\"Describe the surroundings in a friendly, conversational tone, speaking directly to the user. \n",
        "Give practical and helpful information about where they are, what is happening around them, and how busy the area is. \n",
        "If there are moving vehicles, bicycles, or potential dangers, clearly warn the user and gently guide them \n",
        "(for example, tell them to be careful, wait, or stay alert). \n",
        "Avoid tiny details, colors, or technical descriptions, but provide enough context to help the user feel oriented and informed, \n",
        "as if you are a calm and trustworthy companion walking next to them.\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Test: Outdoor Street Scene\n",
        "\n",
        "Test the model with an outdoor urban scene."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "from PIL import Image\n",
        "\n",
        "# Load test image - outdoor urban scene\n",
        "image_url = \"https://heytripster.com/wp-content/uploads/2023/08/Trastevere-rome.jpg\"\n",
        "image = Image.open(requests.get(image_url, stream=True).raw)\n",
        "\n",
        "# Display the test image\n",
        "display(image.resize((400, 300)))\n",
        "\n",
        "# Run test\n",
        "run_and_display(image, USER_PROMPT, SYSTEM_PROMPT, \"Test 1: Outdoor Urban Scene\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Test: Another Street Scene\n",
        "\n",
        "Test with a different urban environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load second test image\n",
        "image_url_2 = \"https://oitheblog.com/wp-content/uploads/2016/03/SAM_7237.jpg\"\n",
        "image_2 = Image.open(requests.get(image_url_2, stream=True).raw)\n",
        "\n",
        "# Display the test image\n",
        "display(image_2.resize((400, 300)))\n",
        "\n",
        "# Run test\n",
        "run_and_display(image_2, USER_PROMPT, SYSTEM_PROMPT, \"Test 2: Urban Street Scene\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Test: Quick Safety Scan\n",
        "\n",
        "Test with a condensed prompt for quick safety assessment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick scan prompt - for rapid safety checks\n",
        "QUICK_SCAN_PROMPT = \"\"\"Quickly scan the environment and report only critical safety information. Focus on:\n",
        "1. Any immediate dangers or obstacles\n",
        "2. Moving vehicles or people on collision paths\n",
        "3. Whether it's safe to proceed forward\n",
        "\n",
        "Keep your response to 2-3 sentences maximum.\"\"\"\n",
        "\n",
        "# Run test\n",
        "run_and_display(image, QUICK_SCAN_PROMPT, SYSTEM_PROMPT, \"Test 3: Quick Safety Scan\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Test: Traffic Safety Check\n",
        "\n",
        "Test with prompts focused on crossing safety."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Traffic safety system prompt\n",
        "TRAFFIC_SYSTEM_PROMPT = \"\"\"You are an AI assistant specialized in traffic safety for visually impaired pedestrians. \n",
        "Your primary focus is keeping the user safe in traffic-heavy environments.\n",
        "\n",
        "Your critical responsibilities:\n",
        "- IMMEDIATELY warn about any approaching vehicles\n",
        "- Clearly state traffic light status (red/green/changing)\n",
        "- Identify safe crossing opportunities\n",
        "- Provide clear WAIT or GO guidance for crossings\"\"\"\n",
        "\n",
        "# Crossing assistance prompt\n",
        "CROSSING_PROMPT = \"\"\"Help me safely cross this street or intersection. Provide:\n",
        "1. Current traffic light status (if visible)\n",
        "2. Any approaching vehicles from any direction\n",
        "3. Clear instruction: WAIT or SAFE TO CROSS\n",
        "\n",
        "Be very clear and direct - safety is the top priority.\"\"\"\n",
        "\n",
        "# Run test\n",
        "run_and_display(image_2, CROSSING_PROMPT, TRAFFIC_SYSTEM_PROMPT, \"Test 4: Traffic Safety / Crossing Check\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Test with Your Own Image\n",
        "\n",
        "Upload your own image to test the visual assistance system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# For Google Colab: Upload your own image\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()\n",
        "# image_path = list(uploaded.keys())[0]\n",
        "# custom_image = Image.open(image_path)\n",
        "\n",
        "# For local testing: Load from path\n",
        "# custom_image = Image.open(\"path/to/your/image.jpg\")\n",
        "\n",
        "# Uncomment and modify the following to test with your image:\n",
        "# run_and_display(custom_image, USER_PROMPT, SYSTEM_PROMPT, \"Custom Image Test\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrates the core visual assistance functionality of Gozumol using the Qwen2-VL model.\n",
        "\n",
        "**Model Characteristics:**\n",
        "- Qwen2-VL-2B-Instruct: Lightweight (2B parameters), fast inference\n",
        "- Good balance between speed and accuracy\n",
        "- Suitable for edge device deployment\n",
        "\n",
        "**Key Features Tested:**\n",
        "- Environment description for outdoor scenes\n",
        "- Quick safety scans for rapid assessment\n",
        "- Traffic and crossing safety checks\n",
        "- Warm, companion-like conversational tone\n",
        "\n",
        "**Comparison with Phi-4:**\n",
        "- Qwen2-VL is smaller and faster\n",
        "- Phi-4 may provide more detailed descriptions\n",
        "- Both work well for real-time assistance"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
