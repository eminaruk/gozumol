{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Gozumol - Phi-4 Multimodal Testing Notebook\n",
        "\n",
        "This notebook demonstrates testing the Phi-4 Multimodal model for visual assistance.\n",
        "\n",
        "**Purpose**: Test the model's ability to describe environments and provide navigation guidance for visually impaired users.\n",
        "\n",
        "## Requirements\n",
        "\n",
        "```bash\n",
        "torch==2.6.0\n",
        "flash_attn==2.7.4.post1\n",
        "transformers==4.48.2\n",
        "accelerate==1.3.0\n",
        "pillow==11.1.0\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install torch==2.6.0\n",
        "!pip install flash_attn==2.7.4.post1\n",
        "!pip install transformers==4.48.2\n",
        "!pip install accelerate==1.3.0\n",
        "!pip install pillow==11.1.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Model and Processor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "import torch\n",
        "import time\n",
        "from PIL import Image\n",
        "from transformers import AutoModelForCausalLM, AutoProcessor, GenerationConfig\n",
        "\n",
        "# Model configuration\n",
        "MODEL_ID = \"microsoft/Phi-4-multimodal-instruct\"\n",
        "\n",
        "# Load processor and model\n",
        "print(\"Loading processor...\")\n",
        "processor = AutoProcessor.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
        "\n",
        "print(\"Loading model...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    device_map=\"cuda\",\n",
        "    torch_dtype=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    attn_implementation=\"flash_attention_2\",\n",
        ").cuda()\n",
        "\n",
        "generation_config = GenerationConfig.from_pretrained(MODEL_ID)\n",
        "print(\"Model loaded successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Define Vision Assistant Interface\n",
        "\n",
        "This function creates the appropriate prompt structure and processes inputs for the Phi-4 model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def vision_assistant_inference(\n",
        "    processor,\n",
        "    model,\n",
        "    system_prompt: str,\n",
        "    content_list: list,\n",
        "    generation_config,\n",
        "    max_new_tokens: int = 1024\n",
        "):\n",
        "    \"\"\"\n",
        "    Generate a response from the Phi-4 multimodal model.\n",
        "\n",
        "    Parameters:\n",
        "        processor: The AutoProcessor for the model\n",
        "        model: The loaded Phi-4 model\n",
        "        system_prompt: System message defining assistant behavior\n",
        "        content_list: List of content items with type, content, and role\n",
        "        generation_config: Generation configuration\n",
        "        max_new_tokens: Maximum tokens to generate\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (formatted_prompt, response_text, timing_info)\n",
        "    \"\"\"\n",
        "    # Token definitions\n",
        "    system_token = \"<|system|>\"\n",
        "    user_token = \"<|user|>\"\n",
        "    assistant_token = \"<|assistant|>\"\n",
        "    end_token = \"<|end|>\"\n",
        "\n",
        "    # Build prompt with system message\n",
        "    complete_prompt = f\"{system_token}{system_prompt}{end_token}\"\n",
        "\n",
        "    # Collect media\n",
        "    images = []\n",
        "    audios = []\n",
        "\n",
        "    # Process content items\n",
        "    current_role = None\n",
        "    role_content = \"\"\n",
        "\n",
        "    for item in content_list:\n",
        "        item_type = item[\"type\"]\n",
        "        item_role = item.get(\"role\", \"user\")\n",
        "\n",
        "        # Handle role transitions\n",
        "        if current_role is not None and current_role != item_role:\n",
        "            role_token = user_token if current_role == \"user\" else assistant_token\n",
        "            complete_prompt += f\"{role_token}{role_content}{end_token}\"\n",
        "            role_content = \"\"\n",
        "\n",
        "        current_role = item_role\n",
        "\n",
        "        # Process content types\n",
        "        if item_type == \"text\":\n",
        "            role_content += item[\"content\"]\n",
        "        elif item_type == \"image\":\n",
        "            image_index = len(images) + 1\n",
        "            role_content += f\"<|image_{image_index}|>\"\n",
        "            images.append(item[\"content\"])\n",
        "        elif item_type == \"audio\":\n",
        "            audio_index = len(audios) + 1\n",
        "            role_content += f\"<|audio_{audio_index}|>\"\n",
        "            audios.append(item[\"content\"])\n",
        "\n",
        "    # Add final role content\n",
        "    if current_role is not None:\n",
        "        role_token = user_token if current_role == \"user\" else assistant_token\n",
        "        complete_prompt += f\"{role_token}{role_content}{end_token}\"\n",
        "\n",
        "    # Add assistant token to prompt response\n",
        "    if current_role != \"assistant\":\n",
        "        complete_prompt += f\"{assistant_token}\"\n",
        "\n",
        "    # Track timing\n",
        "    timing_info = {}\n",
        "\n",
        "    # Process inputs\n",
        "    timing_info[\"start_processor\"] = time.time()\n",
        "    inputs = processor(\n",
        "        text=complete_prompt,\n",
        "        images=images if images else None,\n",
        "        audios=audios if audios else None,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(\"cuda:0\")\n",
        "    timing_info[\"processor_time\"] = time.time() - timing_info[\"start_processor\"]\n",
        "\n",
        "    # Generate response\n",
        "    timing_info[\"start_generation\"] = time.time()\n",
        "    generate_ids = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        generation_config=generation_config,\n",
        "    )\n",
        "    timing_info[\"generation_time\"] = time.time() - timing_info[\"start_generation\"]\n",
        "\n",
        "    # Extract new tokens only\n",
        "    generate_ids = generate_ids[:, inputs[\"input_ids\"].shape[1]:]\n",
        "\n",
        "    # Decode response\n",
        "    timing_info[\"start_decode\"] = time.time()\n",
        "    response = processor.batch_decode(\n",
        "        generate_ids,\n",
        "        skip_special_tokens=True,\n",
        "        clean_up_tokenization_spaces=False\n",
        "    )[0]\n",
        "    timing_info[\"decode_time\"] = time.time() - timing_info[\"start_decode\"]\n",
        "\n",
        "    # Calculate total time\n",
        "    timing_info[\"total_time\"] = (\n",
        "        timing_info[\"processor_time\"] +\n",
        "        timing_info[\"generation_time\"] +\n",
        "        timing_info[\"decode_time\"]\n",
        "    )\n",
        "\n",
        "    return complete_prompt, response, timing_info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Display Helper Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "def run_and_display(system_prompt, content_list, test_name=\"Test\"):\n",
        "    \"\"\"\n",
        "    Run inference and display results in a formatted way.\n",
        "\n",
        "    Parameters:\n",
        "        system_prompt: System message for the assistant\n",
        "        content_list: List of content items\n",
        "        test_name: Name to display for this test\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (prompt, response, timing)\n",
        "    \"\"\"\n",
        "    display(Markdown(f\"## {test_name}\"))\n",
        "\n",
        "    # Show the user prompt\n",
        "    user_text = [item[\"content\"] for item in content_list if item[\"type\"] == \"text\"]\n",
        "    if user_text:\n",
        "        display(Markdown(f\"**User Prompt:** {user_text[-1][:200]}...\"))\n",
        "\n",
        "    # Run inference\n",
        "    prompt, response, timing = vision_assistant_inference(\n",
        "        processor, model, system_prompt, content_list, generation_config\n",
        "    )\n",
        "\n",
        "    # Display response\n",
        "    display(Markdown(\"### Response:\"))\n",
        "    display(Markdown(f\"> {response}\"))\n",
        "\n",
        "    # Display timing\n",
        "    display(Markdown(\"### Timing:\"))\n",
        "    display(Markdown(f\"\"\"\n",
        "- Processor time: {timing['processor_time']:.3f}s\n",
        "- Generation time: {timing['generation_time']:.3f}s\n",
        "- Decode time: {timing['decode_time']:.3f}s\n",
        "- **Total time: {timing['total_time']:.3f}s**\n",
        "    \"\"\"))\n",
        "\n",
        "    return prompt, response, timing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Visual Assistance Prompts\n",
        "\n",
        "These are the prompts optimized for visually impaired navigation assistance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# System prompt for visual assistance\n",
        "SYSTEM_PROMPT = \"\"\"You are an AI assistant guiding a visually impaired person through a live camera feed. \n",
        "Your role is to be a calm, trustworthy companion who helps them navigate safely and independently.\"\"\"\n",
        "\n",
        "# User prompt for environment description\n",
        "USER_PROMPT = \"\"\"Describe the surroundings in a friendly, conversational tone, speaking directly to the user. \n",
        "Give practical and helpful information about where they are, what is happening around them, and how busy the area is. \n",
        "If there are moving vehicles, bicycles, or potential dangers, clearly warn the user and gently guide them \n",
        "(for example, tell them to be careful, wait, or stay alert). \n",
        "Avoid tiny details, colors, or technical descriptions, but provide enough context to help the user feel oriented and informed, \n",
        "as if you are a calm and trustworthy companion walking next to them.\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Test: Outdoor Street Scene\n",
        "\n",
        "Test the model with an outdoor urban scene."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load test image - outdoor urban scene\n",
        "image_url = \"https://heytripster.com/wp-content/uploads/2023/08/Trastevere-rome.jpg\"\n",
        "image = Image.open(requests.get(image_url, stream=True).raw)\n",
        "\n",
        "# Display the test image\n",
        "display(image.resize((400, 300)))\n",
        "\n",
        "# Build content list\n",
        "content_list = [\n",
        "    {\"type\": \"image\", \"content\": image, \"role\": \"user\"},\n",
        "    {\"type\": \"text\", \"content\": USER_PROMPT, \"role\": \"user\"},\n",
        "]\n",
        "\n",
        "# Run test\n",
        "run_and_display(SYSTEM_PROMPT, content_list, \"Test 1: Outdoor Urban Scene\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Test: Another Street Scene\n",
        "\n",
        "Test with a different urban environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load second test image\n",
        "image_url_2 = \"https://oitheblog.com/wp-content/uploads/2016/03/SAM_7237.jpg\"\n",
        "image_2 = Image.open(requests.get(image_url_2, stream=True).raw)\n",
        "\n",
        "# Display the test image\n",
        "display(image_2.resize((400, 300)))\n",
        "\n",
        "# Build content list\n",
        "content_list_2 = [\n",
        "    {\"type\": \"image\", \"content\": image_2, \"role\": \"user\"},\n",
        "    {\"type\": \"text\", \"content\": USER_PROMPT, \"role\": \"user\"},\n",
        "]\n",
        "\n",
        "# Run test\n",
        "run_and_display(SYSTEM_PROMPT, content_list_2, \"Test 2: Urban Street Scene\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Test: Quick Safety Scan\n",
        "\n",
        "Test with a condensed prompt for quick safety assessment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick scan prompt - for rapid safety checks\n",
        "QUICK_SCAN_PROMPT = \"\"\"Quickly scan the environment and report only critical safety information. Focus on:\n",
        "1. Any immediate dangers or obstacles\n",
        "2. Moving vehicles or people on collision paths\n",
        "3. Whether it's safe to proceed forward\n",
        "\n",
        "Keep your response to 2-3 sentences maximum.\"\"\"\n",
        "\n",
        "# Build content list with quick scan prompt\n",
        "content_list_quick = [\n",
        "    {\"type\": \"image\", \"content\": image, \"role\": \"user\"},\n",
        "    {\"type\": \"text\", \"content\": QUICK_SCAN_PROMPT, \"role\": \"user\"},\n",
        "]\n",
        "\n",
        "# Run test\n",
        "run_and_display(SYSTEM_PROMPT, content_list_quick, \"Test 3: Quick Safety Scan\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Test: Traffic Safety Check\n",
        "\n",
        "Test with prompts focused on crossing safety."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Traffic safety system prompt\n",
        "TRAFFIC_SYSTEM_PROMPT = \"\"\"You are an AI assistant specialized in traffic safety for visually impaired pedestrians. \n",
        "Your primary focus is keeping the user safe in traffic-heavy environments.\n",
        "\n",
        "Your critical responsibilities:\n",
        "- IMMEDIATELY warn about any approaching vehicles\n",
        "- Clearly state traffic light status (red/green/changing)\n",
        "- Identify safe crossing opportunities\n",
        "- Provide clear WAIT or GO guidance for crossings\"\"\"\n",
        "\n",
        "# Crossing assistance prompt\n",
        "CROSSING_PROMPT = \"\"\"Help me safely cross this street or intersection. Provide:\n",
        "1. Current traffic light status (if visible)\n",
        "2. Any approaching vehicles from any direction\n",
        "3. Clear instruction: WAIT or SAFE TO CROSS\n",
        "\n",
        "Be very clear and direct - safety is the top priority.\"\"\"\n",
        "\n",
        "# Build content list for crossing assistance\n",
        "content_list_crossing = [\n",
        "    {\"type\": \"image\", \"content\": image_2, \"role\": \"user\"},\n",
        "    {\"type\": \"text\", \"content\": CROSSING_PROMPT, \"role\": \"user\"},\n",
        "]\n",
        "\n",
        "# Run test\n",
        "run_and_display(TRAFFIC_SYSTEM_PROMPT, content_list_crossing, \"Test 4: Traffic Safety / Crossing Check\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Test with Your Own Image\n",
        "\n",
        "Upload your own image to test the visual assistance system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# For Google Colab: Upload your own image\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()\n",
        "# image_path = list(uploaded.keys())[0]\n",
        "# custom_image = Image.open(image_path)\n",
        "\n",
        "# For local testing: Load from path\n",
        "# custom_image = Image.open(\"path/to/your/image.jpg\")\n",
        "\n",
        "# Uncomment and modify the following to test with your image:\n",
        "# content_list_custom = [\n",
        "#     {\"type\": \"image\", \"content\": custom_image, \"role\": \"user\"},\n",
        "#     {\"type\": \"text\", \"content\": USER_PROMPT, \"role\": \"user\"},\n",
        "# ]\n",
        "# run_and_display(SYSTEM_PROMPT, content_list_custom, \"Custom Image Test\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrates the core visual assistance functionality of Gozumol using the Phi-4 Multimodal model.\n",
        "\n",
        "**Key Features Tested:**\n",
        "- Environment description for outdoor scenes\n",
        "- Quick safety scans for rapid assessment\n",
        "- Traffic and crossing safety checks\n",
        "- Warm, companion-like conversational tone\n",
        "- Action-oriented guidance (wait, proceed, be careful)\n",
        "\n",
        "**Next Steps:**\n",
        "- Integrate with real-time camera feeds\n",
        "- Add text-to-speech for audio output\n",
        "- Optimize for edge device deployment\n",
        "- Test with more diverse scenarios (indoor, crowded areas, etc.)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
